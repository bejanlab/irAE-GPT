{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, json, spacy, math\n",
    "import IRAEUtils\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config paramaters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPT_DEPLOYMENT = \"OAI05-GPT35Turbo16-0613_061823\"\n",
    "#GPT_DEPLOYMENT = \"V04-GPT4Turbo-2024-04-09\"\n",
    "GPT_DEPLOYMENT = \"V05-GPT4o\"\n",
    "\n",
    "project_path = os.getenv(\"VU_PROJ_PATH\") + \"immunotoxicity/\"\n",
    "notes_path = project_path + \"in/data4llm/\"\n",
    "\n",
    "annotations_path = project_path  +\"in/data4llm/Map.File-irAELabels.csv\"\n",
    "irae_synsets_path = project_path + \"map_irae_prompt_detail.csv\"\n",
    "path_eval = f\"{project_path}out/llm/eval-note-level/{GPT_DEPLOYMENT}/\"\n",
    "\n",
    "filter_list_irae_full = ['Neuropathy', 'Hypothyroid', 'Myasthenia gravis (MG)', 'Rash', 'Colitis', 'Adrenal insufficiency', 'Hepatitis', 'Arthralgia', 'Duodenitis', 'Pancreatitis', 'Hypophysitis', 'Mucositis', 'Arthritis', 'Pneumonitis', 'Joint pain', 'Fever', 'Myalgia'] \n",
    "list_fname_irae_full = ['FileName'] + filter_list_irae_full\n",
    "\n",
    "llm = AzureOpenAI(\n",
    "  api_key = os.getenv(\"AZURE_OPENAI05_API_KEY\"),  \n",
    "  api_version =  \"2024-07-01-preview\",\n",
    "  azure_endpoint = os.getenv(\"AZURE_OPENAI05_ENDPOINT\")\n",
    ")\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load resources: maps & manual annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## maps: IRAE labes: full | norm -> large categories\n",
    "##\n",
    "dict_map_full2norm = IRAEUtils.read2cols_2dict(f\"{project_path}in/data4llm/IRAE.labels.reverse.csv\", 1, 2, \",\", True)\n",
    "dict_map_norm2large = IRAEUtils.read2cols_2dict(f\"{project_path}out/llm/eval-patient-level/map-specific2generic/IRAE.map.refined-large.03.edits.csv\", 0, 1, \",\", True)\n",
    "dict_map_full2large = dict()\n",
    "for key, value in dict_map_full2norm.items():\n",
    "    if key == 'None':\n",
    "        continue  # Skip this key-value pair\n",
    "    dict_map_full2large[key] = dict_map_norm2large[dict_map_full2norm[key]]\n",
    "\n",
    "list_coll_irae_large = IRAEUtils.read1col_2list_skip1(f\"{project_path}out/llm/eval-patient-level/map-specific2generic/IRAE.map.refined-large.03.edits.csv\", 1, \",\")\n",
    "set_irae_large = set(list_coll_irae_large)\n",
    "sorted_list_irae_large = sorted(set_irae_large)\n",
    "\n",
    "# [2] load df[notes & irAE annotations]\n",
    "#\n",
    "notelist = []\n",
    "with open(annotations_path, 'r') as f:    \n",
    "    next(f) # Skip the first line\n",
    "    for line in f:    \n",
    "        cols = line.split(',')\n",
    "        #print(os.path.join(notes_path, cols[0]))\n",
    "        with open(os.path.join(notes_path, cols[0]), 'r') as datafile:\n",
    "            notelist.append({\"FileName\":cols[0], \"text\":datafile.read()})       \n",
    "df_notes = pd.DataFrame(notelist) \n",
    "df_labels = pd.read_csv(annotations_path)\n",
    "\n",
    "df_gold_full = pd.merge(df_labels, df_notes, on='FileName')\n",
    "df10 = df_gold_full.head(10)\n",
    "#df_gold_full = df_gold_full.head(10)\n",
    "\n",
    "#display(df_gold_full)\n",
    "#display(df_gold_full[list_fname_irae_full])\n",
    "#display(df_gold_full[filter_list_irae_full])\n",
    "#print(df_gold_full[filter_list_irae_full].sum(axis=0))\n",
    "#print(df10[filter_list_irae_full].sum(axis=0))\n",
    "print(f\"Number of notes: {len(df_gold_full)}\")\n",
    "\n",
    "# [3] load irAE sysnset map\n",
    "#\n",
    "list_irae_synsets =  IRAEUtils.read2cols_2list_all(irae_synsets_path, 0, 1, \"|\")\n",
    "\n",
    "dict_irae_synsets = dict()\n",
    "for tuple2 in list_irae_synsets : \n",
    "    dict_irae_synsets[tuple2[0]] = tuple2[1]\n",
    "#print(dict_irae_synsets)\n",
    "\n",
    "# [4] build the list of binary questions in json format that will be included into the prompt\n",
    "#\n",
    "ici_list = \"atezolizumab (tecentriq, atezo), avelumab (bavencio), durvalumab (imfinzi), ipilimumab (yervoy), nivolumab (opdivo, nivo), pembrolizumab (keytruda, pembro)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## irAE counts - note level\n",
    "list_all_irae_full = [irae_label for irae_label in df_gold_full.columns if irae_label not in ['FileName', 'text', 'GRID']]\n",
    "#df_gold_full['GRID'] = df_gold_full['FileName'].apply(lambda x: x.split('.')[3])\n",
    "\n",
    "df_gold_full[list_all_irae_full].sum().to_csv(project_path + \"/out/llm/notes/batches/IRAE.counts.note-level.cohort-note-subset.csv\")\n",
    "\n",
    "display(df_gold_full[list_all_irae_full].sum())\n",
    "\n",
    "filter2_list_irae_full  = []\n",
    "for irae_full in list_all_irae_full:\n",
    "        column_sum = df_gold_full[irae_full].sum()\n",
    "        if column_sum > 0 :\n",
    "                filter2_list_irae_full.append(irae_full)\n",
    "\n",
    "print(f'Total irAE annotated notes: {len(df_gold_full)}')\n",
    "print(list_all_irae_full)\n",
    "print(sorted(filter2_list_irae_full))\n",
    "print(sorted(filter_list_irae_full))\n",
    "\n",
    "#display(df_gold_full[['GRID'] + list_fname_irae_full])\n",
    "#display([list_fname_irae_full])\n",
    "#display(df_gold_full[list_fname_irae_full])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert full to large irAEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## init row dictionaries for converting specific to large irAE categs\n",
    "## \n",
    "def init_dict_large_irae(list_irae_large) : \n",
    "    dict_large_irae = dict()\n",
    "    for irae_large in list_irae_large :\n",
    "        dict_large_irae[irae_large] = 0\n",
    "\n",
    "    return dict_large_irae\n",
    "\n",
    "## convert specific to large irAE patient dataframe\n",
    "## \n",
    "def convert_specific_large(df_irae_full, dict_map_full2large, list_irae_full, list_irae_large) :\n",
    "    df_irae_large = pd.DataFrame(columns = list_irae_large)    \n",
    "\n",
    "    for index, row in df_irae_full.iterrows():\n",
    "        dict_row = init_dict_large_irae(list_irae_large)\n",
    "        for irae_full in list_irae_full :\n",
    "            if row[irae_full] == 1 :\n",
    "                dict_row[dict_map_full2large[irae_full]] = 1\n",
    "                \n",
    "        df_irae_large = pd.concat([df_irae_large, pd.DataFrame([dict_row])], ignore_index=True)\n",
    "    \n",
    "    return df_irae_large\n",
    "\n",
    "## convert gold full irAE to gold large irAE\n",
    "##\n",
    "df_gold_large = convert_specific_large(df_gold_full[filter_list_irae_full], dict_map_full2large, filter_list_irae_full, sorted_list_irae_large)\n",
    "display(df_gold_large)\n",
    "\n",
    "## Build irAE large filter list (exclude null irAEs large labels)\n",
    "##\n",
    "filter_list_irae_large  = []\n",
    "for irae_large in sorted_list_irae_large:\n",
    "        column_sum = df_gold_large[irae_large].sum()\n",
    "        if column_sum > 0 :\n",
    "                filter_list_irae_large.append(irae_large)\n",
    "\n",
    "print(f\"sorted_list_irae_large:{len(sorted_list_irae_large)} -- filter_list_irae_large:{len(filter_list_irae_large)}\")\n",
    "print(set(sorted_list_irae_large) - set(filter_list_irae_large))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IRAE prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_irae_list_json(irae_list):\n",
    "    s = '{'\n",
    "    for irae_label in irae_list :\n",
    "        s += f'''\\n\"{irae_label}\": Output 'Yes' if the patient has experienced {dict_irae_synsets[irae_label]} because of exposure to one or more immune checkpoint inhibitors. Otherwise, output 'No'.,'''\n",
    "    s += '}'\n",
    "\n",
    "    return s\n",
    "\n",
    "print(get_irae_list_json(filter_list_irae_full))\n",
    "\n",
    "def prompt_specific_json(note_text, ici_list, irae_list):\n",
    "    irae_json_format = get_irae_list_json(irae_list)\n",
    "    messages = []\n",
    "    messages.append({\"role\": \"system\", \"content\": f\"\"\"You are a clinical expert in identifying immune-related adverse events (irAEs) caused by immune checkpoint inhibitors (ICIs).                                           \n",
    "                     You will receive as input a patient note corresponding to a patient who was treated or is currently treated with one or multiple immune checkpoint inhibitors (ICIs) from the following ICI list: {ici_list}. \n",
    "                     Your task is to determine if the patient note describes any of the immune-related adverse events (irAEs) experienced by the patient and caused by immune checkpoint inhibitors.\n",
    "                     Output your response in a JSON format using the following structure: \n",
    "                     {irae_json_format}\"\"\"})\n",
    "    messages.append({\"role\": \"user\", \"content\": f\"\"\"Does the following patient note describe immune-related adverse \n",
    "                     events experienced by the patient? \n",
    "                     Patient note: {note_text}\"\"\"})                     \n",
    "    return messages\n",
    "\n",
    "#print(prompt_specific_json(\" .. test .. \", ici_list, filter_list_irae_full))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval + error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-label evaluation - error analysis\n",
    "#df_irae_full_eval = IRAEUtils.irae_eval(y_all_filter, y_llmresponses_filter, filter_list_irae_full)\n",
    "def irae_eval_error(df_files, df_y_gold, df_y_llm, list_irae_label) :\n",
    "    list_precision = []\n",
    "    list_recall = []\n",
    "    list_specificity = []\n",
    "    list_f1 = []\n",
    "    list_acc = []\n",
    "\n",
    "    print(f\"Eval: df_file[{len(df_files)}] df_y_gold[{len(df_y_gold)}] df_y_llm[{len(df_y_llm)}]\")\n",
    "\n",
    "    dict_fp = dict() # key: irAE, vals: list of files\n",
    "    dict_fn = dict() # key: irAE, vals: list of files\n",
    "\n",
    "    for irae in list_irae_label : \n",
    "        dict_fp[irae] = []\n",
    "        dict_fn[irae] = []\n",
    "\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    TN = 0\n",
    "\n",
    "    TP_er = 0\n",
    "    FP_er = 0\n",
    "    FN_er = 0\n",
    "    TN_er = 0\n",
    "\n",
    "    df_eval = pd.DataFrame(columns=['irAE', 'TP', 'FP', 'FN', 'TN', 'Precision', 'Recall', 'Specificity', 'F1', 'Accuracy', 'Support'])\n",
    "\n",
    "    # build contingency tables for each irAE and compute evaluation measures\n",
    "    for index, irae in enumerate(list_irae_label):\n",
    "        y_gold_vector = np.array(df_y_gold[:, index]).astype(int)\n",
    "        y_llm_vector = np.array(df_y_llm[:, index]).astype(int)\n",
    "\n",
    "        TP_local = 0\n",
    "        FP_local = 0\n",
    "        FN_local = 0\n",
    "        TN_local = 0\n",
    "\n",
    "        for file_name, gold_01response, llm_01response in zip(df_files, y_gold_vector, y_llm_vector) :\n",
    "            #print(f\"File {file_name}: Gold{gold_01response} LLM{llm_01response}\")\n",
    "            \n",
    "            if gold_01response == 1 and llm_01response == 1 :\n",
    "                TP_local += 1\n",
    "            \n",
    "            if gold_01response == 0 and llm_01response == 1 :\n",
    "                FP_local += 1                \n",
    "                dict_fp[irae].append(file_name)\n",
    "\n",
    "            if gold_01response == 1 and llm_01response == 0 :\n",
    "                FN_local += 1                \n",
    "                dict_fn[irae].append(file_name)\n",
    "        \n",
    "            if gold_01response == 0 and llm_01response == 0 :\n",
    "                TN_local += 1                                \n",
    "\n",
    "        cm = confusion_matrix(y_gold_vector, y_llm_vector)\n",
    "        #print(cm)\n",
    "\n",
    "        if len(cm) == 1 :\n",
    "            cmTP = cmFP = cmFN = 0\n",
    "            cmTN = cm[0][0]\n",
    "        else :\n",
    "            cmTP = cm[1,1]\n",
    "            cmFP = cm[0,1]\n",
    "            cmFN = cm[1,0]\n",
    "            cmTN = cm[0,0]\n",
    "\n",
    "            TP += cmTP\n",
    "            FP += cmFP\n",
    "            FN += cmFN\n",
    "            TN += cmTN\n",
    "\n",
    "        cm_positives = cmTP+cmFN\n",
    "        cm_negatives = cmFP+cmTN\n",
    "        cm_total = cm_positives + cm_negatives\n",
    "\n",
    "        if cmTP + cmFP == 0 : cm_precision = 0\n",
    "        else : cm_precision = cmTP / (cmTP + cmFP)                \n",
    "\n",
    "        if cmTP + cmFN == 0 : cm_recall = 0\n",
    "        else : cm_recall = cmTP / (cmTP + cmFN)\n",
    "\n",
    "        if cmTN + cmFP == 0 : cm_specificity = 0\n",
    "        else: cm_specificity = cmTN / (cmTN + cmFP)\n",
    "\n",
    "        if cm_precision + cm_recall == 0 : cm_f1 = 0\n",
    "        else : cm_f1 = 2 * cm_precision * cm_recall / (cm_precision + cm_recall)\n",
    "\n",
    "        if cm_total == 0 : cm_acc = 0\n",
    "        else : cm_acc = (cmTP + cmTN) / cm_total\n",
    "\n",
    "        if math.isnan(cm_precision): cm_precision = 0\n",
    "        if math.isnan(cm_recall): cm_recall = 0\n",
    "        if math.isnan(cm_specificity): cm_specificity = 0\n",
    "        if math.isnan(cm_f1): cm_f1 = 0\n",
    "        if math.isnan(cm_acc): cm_acc = 0\n",
    "\n",
    "        list_precision.append(cm_precision)\n",
    "        list_recall.append(cm_recall)\n",
    "        list_specificity.append(cm_specificity)\n",
    "        list_f1.append(cm_f1)\n",
    "        list_acc.append(cm_acc)\n",
    "        \n",
    "        print(f\"irAE[{irae}] TP[{cmTP}][{TP_local}] FP[{cmFP}][{FP_local}] FN[{cmFN}][{FN_local}] TN[{cmTN}][{TN_local}] \")\n",
    "        print(cm)\n",
    "\n",
    "        df_row = {'irAE' : irae, 'TP' : cmTP, 'FP' : cmFP, 'FN' : cmFN, 'TN' : cmTN, 'Precision' : cm_precision, 'Recall' : cm_recall, 'Specificity' : cm_specificity, 'F1' : cm_f1, 'Accuracy' : cm_acc, 'Support' :cm_positives}    \n",
    "        df_eval = pd.concat([df_eval, pd.DataFrame([df_row])], ignore_index=True)    \n",
    "\n",
    "    # macro/micro averaged results\n",
    "    Positives = TP + FN\n",
    "    Negatives = FP + TN\n",
    "    Total = Positives + Negatives\n",
    "\n",
    "    if TP + FP == 0 : microPrecision = 0\n",
    "    else : microPrecision = TP / (TP + FP)\n",
    "\n",
    "    if TP + FN == 0 : microRecall = 0\n",
    "    else : microRecall = TP / (TP + FN)\n",
    "\n",
    "    if TN + FP == 0 : microSpecificity = 0\n",
    "    else : microSpecificity = TN / (TN + FP)\n",
    "\n",
    "    if microPrecision + microRecall == 0 : microF1 = 0\n",
    "    else : microF1 = 2 * microPrecision * microRecall / (microPrecision + microRecall)\n",
    "\n",
    "    if Total == 0 : microAcc = 0\n",
    "    else : microAcc = (TP + TN) / Total\n",
    "\n",
    "    if math.isnan(microPrecision): microPrecision = 0\n",
    "    if math.isnan(microRecall): microRecall = 0\n",
    "    if math.isnan(microSpecificity): microSpecificity = 0\n",
    "    if math.isnan(microF1): microF1 = 0\n",
    "    if math.isnan(microAcc): microAcc = 0\n",
    "\n",
    "    # add empty row\n",
    "    df_row = {'irAE' : '', 'TP' : '', 'FP' : '', 'FN' : '', 'TN' :'', 'Precision' : '', 'Recall' : '', 'Specificity' : '', 'F1' : '', 'Accuracy' : '', 'Support' : ''}\n",
    "    df_eval = pd.concat([df_eval, pd.DataFrame([df_row])], ignore_index=True)\n",
    "\n",
    "    ## micro-average\n",
    "    df_row = {'irAE' : 'micro avg', 'TP': TP, 'FP' : FP, 'FN' : FN, 'TN' : TN, 'Precision' : microPrecision, 'Recall' : microRecall, 'Specificity' : microSpecificity, 'F1' : microF1, 'Accuracy' : microAcc, 'Support' : Positives}\n",
    "    df_eval = pd.concat([df_eval, pd.DataFrame([df_row])], ignore_index=True)\n",
    "\n",
    "    ## macro-average\n",
    "    df_row = {'irAE' : 'macro avg', 'TP': TP, 'FP' : FP, 'FN' : FN, 'TN' : TN, 'Precision' : np.mean(list_precision), 'Recall' : np.mean(list_recall), 'Specificity' : np.mean(list_specificity), 'F1' : np.mean(list_f1), 'Accuracy' : np.mean(list_acc), 'Support' : Positives}\n",
    "    df_eval = pd.concat([df_eval, pd.DataFrame([df_row])], ignore_index=True)\n",
    "\n",
    "    return df_eval, dict_fp, dict_fn\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM output processisng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Returns <Flag , list01> True if the LLM response is successfully parsed\n",
    "##\n",
    "def convert_yn_dict_to_01_list(dict_irae_reponse, irae_list, nlp) :\n",
    "    response_01_list = []\n",
    "    for irae_elem in irae_list :\n",
    "        response_01_elem = IRAEUtils.convert_llm_response_to_01(dict_irae_reponse[irae_elem], nlp)\n",
    "        #print(f\"{irae_elem} : {dict_irae_reponse[irae_elem]} : {response_01_elem}\")\n",
    "        response_01_list.append(response_01_elem)\n",
    "        if response_01_elem == -1 :\n",
    "            return False, response_01_list\n",
    "    \n",
    "    return True, response_01_list\n",
    "\n",
    "## Filter out data points corresponding to invalid LLM results \n",
    "##\n",
    "def filter_invalid_llm_responses(files_gold, y_gold, y_llm_yn, irae_list, nlp) :\n",
    "    #print(f\"before: filter_invalid_llm_responses: {len(y_gold)} <> {len(y_llm_yn)}\")\n",
    "    \n",
    "    files_gold_filter = []\n",
    "    y_gold_filter = np.empty((0, len(irae_list)))\n",
    "    y_llm_filter = np.empty((0, len(irae_list)))\n",
    "\n",
    "    for row_file_gold, row_y_gold, llm_response_yn in zip(files_gold, y_gold, y_llm_yn) :\n",
    "        #print(f\"\\nrow_y_gold({row_y_gold})\")\n",
    "        #print(f\"llm_response_yn({llm_response_yn})\")\n",
    "        json_llm_response_yn = llm_response_yn.removeprefix(\"```json\").removesuffix(\"```\").strip()\n",
    "        if IRAEUtils.is_json(json_llm_response_yn) :\n",
    "            dict_irae_reponse = json.loads(json_llm_response_yn)\n",
    "            flag, llm_response_01 = convert_yn_dict_to_01_list(dict_irae_reponse, irae_list, nlp)\n",
    "\n",
    "            if flag == True :\n",
    "                #print(f\"llm_response_01({llm_response_01})\")\n",
    "                files_gold_filter.append(row_file_gold)\n",
    "                y_gold_filter = np.vstack([y_gold_filter, row_y_gold])\n",
    "                y_llm_filter = np.vstack([y_llm_filter, llm_response_01])\n",
    "\n",
    "                #print(f\"\\ny_gold_filter({y_gold_filter})\")\n",
    "                #print(f\"y_llm_filter({y_llm_filter})\")\n",
    "\n",
    "    #print(f\"after: filter_invalid_llm_responses: {len(y_gold_filter)} <> {len(y_llm_filter)}\")\n",
    "    return files_gold_filter, y_gold_filter, y_llm_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all = df10[[\"text\"]].values\n",
    "y_all = df10[filter_list_irae_full].values\n",
    "F_all = df10[[\"FileName\"]].values\n",
    "\n",
    "#X_all = df_gold_full[[\"text\"]].values\n",
    "#y_all = df_gold_full[filter_list_irae_full].values\n",
    "#F_all = df_gold_full[[\"FileName\"]].values\n",
    "\n",
    "exception_list = []\n",
    "y_llmresponses = []\n",
    "\n",
    "# Run the LLM for each note in the dataset and collect its response\n",
    "#for note in X_all:\n",
    "for index, note in enumerate(X_all):\n",
    "    try:\n",
    "        llm_response = llm.chat.completions.create(model = GPT_DEPLOYMENT,\n",
    "            temperature=0.0, max_tokens=500, n = 1,\n",
    "            frequency_penalty=0, presence_penalty=0, seed = 13,     \n",
    "            #top_p=1, ## reco: alter this param or temp but not both https://platform.openai.com/docs/api-reference/chat/create            \n",
    "            #messages = prompt_func(note))\n",
    "            messages = prompt_specific_json(note, ici_list, filter_list_irae_full))                                \n",
    "        \n",
    "        print(\"Note: \"+str(index))\n",
    "        #print(\"Prompt: \"+str(prompt_func(note, ici_list, irae_list)))\n",
    "        print(llm_response.choices[0].message.content.strip())\n",
    "        y_llmresponses.append(llm_response.choices[0].message.content.strip())\n",
    "        #print(response)\n",
    "        #print('.', end='', flush=True)\n",
    "    except Exception as e:            \n",
    "        print(\"LLMException: \"+str(e).strip().replace('\\n', ' '))\n",
    "        y_llmresponses.append(\"LLMException: \"+str(e).strip().replace('\\n', ' '))\n",
    "        exception_list.append(\"LLMException: \"+str(e).strip().replace('\\n', ' '))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation [full]: filtered irAE full labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation at irAE type level\n",
    "\n",
    "F_all_filter, y_all_filter, y_llmresponses_filter = filter_invalid_llm_responses(F_all, y_all, y_llmresponses, filter_list_irae_full, nlp)\n",
    "\n",
    "clf_report = classification_report(y_all_filter, y_llmresponses_filter, target_names = filter_list_irae_full, zero_division=0, output_dict=True)\n",
    "df_clf_report = pd.DataFrame(clf_report).transpose()\n",
    "display(df_clf_report)\n",
    "\n",
    "df_irae_full_eval = IRAEUtils.irae_eval(y_all_filter, y_llmresponses_filter, filter_list_irae_full)\n",
    "display(df_irae_full_eval)\n",
    "\n",
    "df_clf_report.to_csv(f\"{path_eval}EVAL-FULL.CLF-REPORT.{GPT_DEPLOYMENT}.csv\", index=True)\n",
    "df_irae_full_eval.to_csv(f\"{path_eval}EVAL-FULL.DETAILED-REPORT.{GPT_DEPLOYMENT}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_irae_full_eval, dict_full_fp, dict_full_fn  = irae_eval_error(F_all_filter, y_all_filter, y_llmresponses_filter, filter_list_irae_full)\n",
    "\n",
    "display(df_irae_full_eval)\n",
    "\n",
    "with open(f\"{path_eval}ErrorAnalysis_FP_NEW.csv\", \"w\") as file:\n",
    "    # Write each item in the list to the file\n",
    "    for irae in filter_list_irae_full:\n",
    "        file.write(f\"\\n\\nirAE: {irae}\\n\")\n",
    "        for file_name in dict_full_fp[irae]:\n",
    "            file.write(f\"\\n{file_name}\")\n",
    "        \n",
    "with open(f\"{path_eval}ErrorAnalysis_FN_NEW.csv\", \"w\") as file:\n",
    "    # Write each item in the list to the file\n",
    "    for irae in filter_list_irae_full:\n",
    "        file.write(f\"\\n\\nirAE: {irae}\\n\")\n",
    "        for file_name in dict_full_fn[irae]:\n",
    "            file.write(f\"\\n{file_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation [large]: filtered irAE large labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation at irAE category level\n",
    "\n",
    "## Convert np.array with binary labels for irAE full to df with binary labels for irAE large\n",
    "##\n",
    "def convert_np_full_to_df_large(np_irae_full, dict_map_full2large, list_irae_full, list_irae_large) :\n",
    "    df_irae_large = pd.DataFrame(columns = list_irae_large)\n",
    "     \n",
    "    for row in np_irae_full :\n",
    "        dict_row = init_dict_large_irae(list_irae_large)\n",
    "        for index_irae_full, label_irae_full in enumerate(list_irae_full):\n",
    "            if row[index_irae_full] == 1 :\n",
    "                    dict_row[dict_map_full2large[label_irae_full]] = 1\n",
    "\n",
    "        df_irae_large = pd.concat([df_irae_large, pd.DataFrame([dict_row])], ignore_index=True)\n",
    "    \n",
    "    return df_irae_large\n",
    "\n",
    "\n",
    "df_y_all_filter = convert_np_full_to_df_large(y_all_filter, dict_map_full2large, filter_list_irae_full, filter_list_irae_large)\n",
    "df_y_llmresponses_filter = convert_np_full_to_df_large(y_llmresponses_filter, dict_map_full2large, filter_list_irae_full, filter_list_irae_large)\n",
    "\n",
    "np_y_all_filter = df_y_all_filter.to_numpy().astype(int)\n",
    "np_y_llmresponses_filter = df_y_llmresponses_filter.to_numpy().astype(int)\n",
    "\n",
    "final_clf_report = classification_report(np_y_all_filter, np_y_llmresponses_filter, target_names = filter_list_irae_large, zero_division=0, output_dict=True)\n",
    "final_clf_report = pd.DataFrame(final_clf_report).transpose()\n",
    "display(final_clf_report)\n",
    "\n",
    "final_df_irae_large_eval = IRAEUtils.irae_eval(np_y_all_filter, np_y_llmresponses_filter, filter_list_irae_large)\n",
    "display(final_df_irae_large_eval)\n",
    "\n",
    "final_clf_report.to_csv(f\"{path_eval}EVAL-LARGE.CLF-REPORT.{GPT_DEPLOYMENT}.csv\", index=True)\n",
    "final_df_irae_large_eval.to_csv(f\"{path_eval}EVAL-LARGE.DETAILED-REPORT.{GPT_DEPLOYMENT}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test tiktoken: print tokens and their corresponding parts of the text\n",
    "\n",
    "s = \"Output 'Yes' if the patient has experienced Neuropathy (Neurotox, Neurotoxicity) because of\"\n",
    "s2 = \"Rash\"\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "tokens = encoding.encode(s2)\n",
    "\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Decoded tokens:\", [encoding.decode([token]) for token in tokens])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
