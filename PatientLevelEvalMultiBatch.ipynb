{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from openai import AzureOpenAI                      \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys, time, random, math, os, json, spacy, re\n",
    "import matplotlib.pyplot as plt\n",
    "import IRAEUtils\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, make_scorer, roc_auc_score, roc_curve\n",
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## config params\n",
    "##\n",
    "#GPT_DEPLOYMENT = \"OAI05-GPT35Turbo16-0613_061823\"\n",
    "#GPT_DEPLOYMENT = \"V04-GPT4Turbo-2024-04-09\"\n",
    "GPT_DEPLOYMENT = \"V05-GPT4o\"\n",
    "\n",
    "IRAE_LABEL_COUNT_MAX = 101\n",
    "\n",
    "list_batch = ['70', '71', '72', '73', '01', '02', '03', '04', '07', '08']\n",
    "\n",
    "## paths\n",
    "##\n",
    "project_path = os.getenv(\"VU_PROJ_PATH\") + \"immunotoxicity/\"\n",
    "notes_path = project_path + \"out/llm/notes/\" \n",
    "irae_anno_map_path = f\"{project_path}out/llm/notes/batches/frozen/\"\n",
    "path_eval = f\"{project_path}out/llm/eval-patient-level/{GPT_DEPLOYMENT}/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load resources: maps & manual annotations (df_gold_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## maps: IRAE labes: full | norm -> large categories\n",
    "##\n",
    "dict_map_full2norm = IRAEUtils.read2cols_2dict(f\"{project_path}in/data4llm/IRAE.labels.reverse.csv\", 1, 2, \",\", True)\n",
    "dict_map_norm2large = IRAEUtils.read2cols_2dict(f\"{project_path}out/llm/eval-patient-level/map-specific2generic/IRAE.map.refined-large.03.edits.csv\", 0, 1, \",\", True)\n",
    "dict_map_full2large = dict()\n",
    "for key, value in dict_map_full2norm.items():\n",
    "    if key == 'None':\n",
    "        continue  # Skip this key-value pair\n",
    "    dict_map_full2large[key] = dict_map_norm2large[dict_map_full2norm[key]]\n",
    "\n",
    "list_coll_irae_large = IRAEUtils.read1col_2list_skip1(f\"{project_path}out/llm/eval-patient-level/map-specific2generic/IRAE.map.refined-large.03.edits.csv\", 1, \",\")\n",
    "set_irae_large = set(list_coll_irae_large)\n",
    "sorted_list_irae_large = sorted(set_irae_large)\n",
    "\n",
    "#print(dict_map_full2norm)\n",
    "#print(dict_map_norm2large)\n",
    "\n",
    "#print(f\"dict_map_full2norm: {len(dict_map_full2norm)} dict_map_norm2large: {len(dict_map_norm2large)}\")\n",
    "#print(f\"dict_map_full2norm: {dict_map_full2norm}\")\n",
    "#print(f\"dict_map_norm2large: {dict_map_norm2large}\")\n",
    "#print(f\"dict_map_full2large: {dict_map_full2large}\")\n",
    "#print(f\"dict_map_full2norm: {len(dict_map_full2norm)} dict_map_norm2large: {len(dict_map_norm2large)}\")\n",
    "\n",
    "## Load manual annotations + patient characteristics (concatenate batches)\n",
    "##\n",
    "## Header: 'GRID', 'PID', 'AgeFirstICI', 'RaceEth', 'Gender', 'ICIType', 'CancerType', 'FolderName', [irAEs - full labels]\n",
    "df_gold_full = pd.read_csv(f\"{irae_anno_map_path}Map.person-level.Batch.{list_batch[0]}.irAE.csv\")\n",
    "for batch in list_batch[1:]:\n",
    "    df_batch_gold = pd.read_csv(f\"{irae_anno_map_path}Map.person-level.Batch.{batch}.irAE.csv\")\n",
    "    df_gold_full = pd.concat([df_gold_full, df_batch_gold], ignore_index=True)\n",
    "#display(df_gold_full)\n",
    "\n",
    "## Build irAE full list\n",
    "##\n",
    "list_irae_full = df_gold_full.columns.tolist()\n",
    "list_irae_full = list_irae_full[8:] # exclude non-irAE data\n",
    "\n",
    "## Build irAE full filter list (exclude null irAEs)\n",
    "##\n",
    "filter_list_irae_full  = []\n",
    "for irae_full in list_irae_full:\n",
    "        column_sum = df_gold_full[irae_full].sum()\n",
    "        if column_sum > 0 :\n",
    "                filter_list_irae_full.append(irae_full)\n",
    "\n",
    "print(f\"list_irae_full:{len(list_irae_full)} -- filter_list_irae_full:{len(filter_list_irae_full)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert full to large irAEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## init row dictionaries for converting specific to large irAE categs\n",
    "## \n",
    "def init_dict_large_irae(list_irae_large) : \n",
    "    dict_large_irae = dict()\n",
    "    for irae_large in list_irae_large :\n",
    "        dict_large_irae[irae_large] = 0\n",
    "\n",
    "    return dict_large_irae\n",
    "\n",
    "## convert specific to large irAE patient dataframe\n",
    "## \n",
    "def convert_specific_large(df_irae_full, dict_map_full2large, list_irae_full, list_irae_large) :\n",
    "    df_irae_large = pd.DataFrame(columns = list_irae_large)    \n",
    "\n",
    "    for index, row in df_irae_full.iterrows():\n",
    "        dict_row = init_dict_large_irae(list_irae_large)\n",
    "        for irae_full in list_irae_full :\n",
    "            if row[irae_full] == 1 :\n",
    "                dict_row[dict_map_full2large[irae_full]] = 1\n",
    "                \n",
    "        df_irae_large = pd.concat([df_irae_large, pd.DataFrame([dict_row])], ignore_index=True)\n",
    "    \n",
    "    return df_irae_large\n",
    "\n",
    "## convert gold full irAE to gold large irAE\n",
    "##\n",
    "df_gold_large = convert_specific_large(df_gold_full[list_irae_full], dict_map_full2large, list_irae_full, sorted_list_irae_large)\n",
    "#display(df_gold_large)\n",
    "\n",
    "## Build irAE large filter list (exclude null irAEs large labels)\n",
    "##\n",
    "filter_list_irae_large  = []\n",
    "for irae_large in sorted_list_irae_large:\n",
    "        column_sum = df_gold_large[irae_large].sum()\n",
    "        if column_sum > 0 :\n",
    "                filter_list_irae_large.append(irae_large)\n",
    "\n",
    "print(f\"sorted_list_irae_large:{len(sorted_list_irae_large)} -- filter_list_irae_large:{len(filter_list_irae_large)}\")\n",
    "print(set(sorted_list_irae_large) - set(filter_list_irae_large))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 1: Dataset characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract: number of notes/patients/notes per patient\n",
    "##\n",
    "def dataset_size() :\n",
    "    folder_size = 0\n",
    "    for folder_name in df_gold_full['FolderName'].tolist() :\n",
    "        folder_size += len(IRAEUtils.file_names(notes_path + \"/\" + folder_name))\n",
    "    return folder_size\n",
    "\n",
    "total_notes = dataset_size()\n",
    "total_patients = len(df_gold_full)\n",
    "\n",
    "print(f\"Total patients = {total_patients}\")\n",
    "print(f\"Total notes = {total_notes}\")\n",
    "print(f\"Notes per patient = {total_notes/total_patients}\")\n",
    "\n",
    "# Age at first ICI\n",
    "#print(df_all_gold['AgeFirstICI'].describe())\n",
    "print(f\"Age at first ICI, mean={df_gold_full['AgeFirstICI'].mean()} std={df_gold_full['AgeFirstICI'].std()}\")\n",
    "\n",
    "# Gender\n",
    "display(IRAEUtils.df_count_perc2(df_gold_full, 'Gender'))\n",
    "\n",
    "# RaceEth\n",
    "display(IRAEUtils.df_count_perc2(df_gold_full, 'RaceEth'))\n",
    "\n",
    "# CancerType\n",
    "display(IRAEUtils.df_count_perc2(df_gold_full, 'CancerType'))\n",
    "\n",
    "# ICIType\n",
    "display(IRAEUtils.df_count_perc2(df_gold_full, 'ICIType'))\n",
    "\n",
    "#print(df_gold_all[list_irae_label].sum(axis=0))\n",
    "\n",
    "#print(df_gold_large[sorted_list_irae_large].sum(axis=0))\n",
    "\n",
    "\n",
    "# Count patients with no irAEs (ie, with 'None' label)\n",
    "series_irae_large_count = df_gold_large[sorted_list_irae_large].sum(axis=1)\n",
    "none_count = 0\n",
    "for irae_count in series_irae_large_count:\n",
    "    if irae_count == 0 :\n",
    "        none_count += 1\n",
    "\n",
    "# irAE full counts distribution\n",
    "irae_full_sums = df_gold_full[filter_list_irae_full].sum(axis=0)\n",
    "irae_full_sums_df = pd.DataFrame(irae_full_sums, columns=['Count'])\n",
    "row = pd.DataFrame({'Count': none_count}, index=['None'])\n",
    "irae_full_sums_df = pd.concat([irae_full_sums_df, row], ignore_index=False)   \n",
    "irae_full_sums_df['Percentage'] = (irae_full_sums_df['Count'] / total_patients) * 100\n",
    "display(irae_full_sums_df)\n",
    "irae_full_sums_df.to_csv(project_path + \"/out/llm/notes/batches/IRAE.counts.patient-level.cohort-patient-subset.csv\")\n",
    "\n",
    "# irAE large counts distribution\n",
    "irae_large_sums = df_gold_large[sorted_list_irae_large].sum(axis=0)\n",
    "irae_large_sums_df = pd.DataFrame(irae_large_sums, columns=['Count'])\n",
    "row = pd.DataFrame({'Count': none_count}, index=['None'])\n",
    "irae_large_sums_df = pd.concat([irae_large_sums_df, row], ignore_index=False)   \n",
    "\n",
    "irae_large_sums_df['Percentage'] = (irae_large_sums_df['Count'] / total_patients) * 100\n",
    "\n",
    "# Calculate cumulative sum and add as a new row\n",
    "count_sum = irae_large_sums_df['Count'].sum()\n",
    "perc_sum = irae_large_sums_df['Percentage'].sum()\n",
    "row = pd.DataFrame({'Count': count_sum, 'Percentage':perc_sum}, index=['irAE large Total'])\n",
    "irae_large_sums_df = pd.concat([irae_large_sums_df, row], ignore_index=False)   \n",
    "\n",
    "display(irae_large_sums_df)\n",
    "\n",
    "\n",
    "df_map_full2large = pd.DataFrame.from_dict(dict_map_full2large, orient='index')\n",
    "df_sorted_map_full2large = df_map_full2large.sort_index()\n",
    "#display(df_sorted_map_full2large)\n",
    "\n",
    "# Categ - irAE map\n",
    "rows = [{'value': v, 'keys': [k for k, val in dict_map_full2large.items() if val == v]} for v in set(dict_map_full2large.values())]\n",
    "df = pd.DataFrame(rows)\n",
    "df = df.sort_values(by='value')\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversion from note-level llm predictions to patient-level "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## generalization of collapse_llresponses01_notes_to_patient for multiple threshold values\n",
    "## in: df with note level llm predictions\n",
    "## out: list where each element is a df with patient-level llm predictions for a given threshold\n",
    "def collapse_llresponses01_notes_to_patient_multi_th(patient_multirow_df_llmresponse_01) :\n",
    "    # init the list of dictionaries with patient-level predictions\n",
    "    list_dict_collapsed_llmrespose01_multi_th = []    \n",
    "    for _ in range(IRAE_LABEL_COUNT_MAX):        \n",
    "        list_dict_collapsed_llmrespose01_multi_th.append({})\n",
    "    \n",
    "    # update the list of dictionaries\n",
    "    for colname in patient_multirow_df_llmresponse_01.columns:\n",
    "        column_sum = patient_multirow_df_llmresponse_01[colname].sum()\n",
    "        for threshold in range(IRAE_LABEL_COUNT_MAX):\n",
    "            list_dict_collapsed_llmrespose01_multi_th[threshold][colname] = 1 if column_sum > threshold else 0\n",
    "\n",
    "    # init the list of 1-row data frames\n",
    "    list_patient_onerow_df_llmresponse01_multi_th = []\n",
    "    for threshold in range(IRAE_LABEL_COUNT_MAX):\n",
    "        list_patient_onerow_df_llmresponse01_multi_th.append(pd.DataFrame([list_dict_collapsed_llmrespose01_multi_th[threshold]]))\n",
    "\n",
    "    return list_patient_onerow_df_llmresponse01_multi_th\n",
    "\n",
    "# collect llm predictions at note level\n",
    "list_all_notelevel_files = []\n",
    "for batch in list_batch :\n",
    "    pattern = fr'^.*\\.note-level-llm-01\\.B\\.{batch}\\.csv$'\n",
    "    notelevel_files = [(batch, f) for f in os.listdir(f\"{path_eval}final-batches/batch-{batch}/\") if re.match(pattern, f)] # Extract all file names that match the regex pattern\n",
    "    list_all_notelevel_files.extend(notelevel_files)\n",
    "#print(list_all_notelevel_files)\n",
    "\n",
    "# build dict: grid - file_name (note_level)\n",
    "dict_grid_notelevelfile = dict()\n",
    "for batch, file_name in list_all_notelevel_files:\n",
    "    toks = file_name.split(\".\")\n",
    "    grid = toks[2] \n",
    "    dict_grid_notelevelfile[grid] = (batch, file_name)\n",
    "#print(dict_grid_notelevelfile)\n",
    "print(f\"dict_grid_notelevelfile:{len(dict_grid_notelevelfile)}\")\n",
    "\n",
    "# compute llm predictions at patient level - build {grid - multi_llm_patient} dict \n",
    "# Note: account only for the irAEs in the gold dataset (ie, with at least one label) by using filter_list_irae_full\n",
    "dict_grid_df_multi_patientlevel = dict()\n",
    "for index, row in df_gold_full.iterrows():\n",
    "    grid = row['GRID']\n",
    "    batch, file_name = dict_grid_notelevelfile[grid]\n",
    "    df_note_level = pd.read_csv(f\"{path_eval}final-batches/batch-{batch}/{file_name}\")    \n",
    "    list_patient_onerow_df_llmresponse01_multi_th = collapse_llresponses01_notes_to_patient_multi_th(df_note_level[filter_list_irae_full])\n",
    "    dict_grid_df_multi_patientlevel[grid] = list_patient_onerow_df_llmresponse01_multi_th\n",
    "\n",
    "print(f\"dict_grid_df_multi_patientlevel:{len(dict_grid_df_multi_patientlevel)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation: filtered irAE full labels + all threshold values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_y_gold = df_gold_full[filter_list_irae_full].to_numpy().astype(int)\n",
    "\n",
    "df_micro_full = pd.DataFrame(columns=['TH', 'precision', 'recall', 'f1-score', 'support'])\n",
    "for threshold in range(IRAE_LABEL_COUNT_MAX):\n",
    "    df_threshold = pd.DataFrame(columns = filter_list_irae_full)\n",
    "    for index, row in df_gold_full.iterrows():\n",
    "        GRID = row['GRID']\n",
    "        df_threshold = pd.concat([df_threshold, dict_grid_df_multi_patientlevel[GRID][threshold]], ignore_index=True)\n",
    "\n",
    "    df_y_llm = df_threshold.to_numpy().astype(int)    \n",
    "    \n",
    "    final_clf_report = classification_report(df_y_gold, df_y_llm, target_names = filter_list_irae_full, zero_division=0, output_dict=True)\n",
    "    dict_row = {'TH' : threshold}\n",
    "    dict_row.update(final_clf_report['micro avg'])\n",
    "    #print(dict_row)\n",
    "    df_micro_full = pd.concat([df_micro_full, pd.DataFrame([dict_row])], ignore_index=True)\n",
    "    #print(f\"Eval threshold {threshold}:\")\n",
    "    #print(final_clf_report['micro avg'])\n",
    "\n",
    "display(df_micro_full)\n",
    "df_micro_full.to_csv(f\"{path_eval}PatientEval.micro.all_th.irAE-full-filter-{len(filter_list_irae_full)}.{GPT_DEPLOYMENT}.csv\", index=False)\n",
    "\n",
    "# Find the maximum F1 score and its corresponding threshold\n",
    "max_f1_idx = df_micro_full['f1-score'].idxmax()\n",
    "max_f1 = df_micro_full['f1-score'].max()\n",
    "max_f1_threshold = df_micro_full['TH'][max_f1_idx]\n",
    "\n",
    "IRAEUtils.write(f\"{path_eval}PatientEval.best-th.{GPT_DEPLOYMENT}.txt\", f\"max_f1_idx:{max_f1_idx} max_micro-f1:{max_f1} max_micro-f1_threshold:{max_f1_threshold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trends of micro-averaged precision, recall, and F1 scores achieved by the GPT model for various threshold values\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot each metric\n",
    "plt.plot(df_micro_full['TH'], df_micro_full['precision'], marker='o', label='Micro-Precision')\n",
    "plt.plot(df_micro_full['TH'], df_micro_full['recall'], marker='o', label='Micro-Recall')\n",
    "plt.plot(df_micro_full['TH'], df_micro_full['f1-score'], marker='o', label='Micro-F1')\n",
    "\n",
    "# Highlight the max F1 score\n",
    "plt.scatter(max_f1_threshold, max_f1, color='red', s=100, label=f'Best Micro-F1 ({max_f1:.2f})', edgecolors='black', zorder=5)\n",
    "\n",
    "#plt.title(GPT_DEPLOYMENT)\n",
    "#plt.title(\"GPT-3.5\", fontsize=18)\n",
    "#plt.title(\"GPT-4\", fontsize=18)\n",
    "plt.title(\"GPT-4o\", fontsize=18)\n",
    "plt.xlabel('Threshold', fontsize=16)\n",
    "plt.ylabel('Score', fontsize=16)\n",
    "plt.xlim(0,100)\n",
    "plt.xticks(fontsize=13)  \n",
    "plt.yticks(fontsize=13)  \n",
    "plt.grid(True)\n",
    "plt.legend(fontsize=13)\n",
    "\n",
    "plt.savefig(f\"{path_eval}PatientEval.score-trends.{GPT_DEPLOYMENT}.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(f\"max_f1_idx:{max_f1_idx} max_f1:{max_f1} max_f1_threshold:{max_f1_threshold}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation [full]: filtered irAE full labels + best threshold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract detailed results for a given threshold\n",
    "df_y_gold = df_gold_full[filter_list_irae_full].to_numpy().astype(int)\n",
    "\n",
    "best_threshold = max_f1_threshold\n",
    "df_llm_full_best_threshold = pd.DataFrame(columns = filter_list_irae_full)\n",
    "for index, row in df_gold_full.iterrows():\n",
    "    GRID = row['GRID']\n",
    "    df_llm_full_best_threshold = pd.concat([df_llm_full_best_threshold, dict_grid_df_multi_patientlevel[GRID][best_threshold]], ignore_index=True)\n",
    "\n",
    "df_y_llm = df_llm_full_best_threshold.to_numpy().astype(int)    \n",
    "\n",
    "final_clf_report = classification_report(df_y_gold, df_y_llm, target_names = filter_list_irae_full, zero_division=0, output_dict=True)\n",
    "final_clf_report = pd.DataFrame(final_clf_report).transpose()\n",
    "display(final_clf_report)\n",
    "\n",
    "final_df_irae_eval = IRAEUtils.irae_eval(df_y_gold, df_y_llm, filter_list_irae_full)\n",
    "display(final_df_irae_eval)\n",
    "\n",
    "final_clf_report.to_csv(f\"{path_eval}PATIENT-EVAL.FULL.CLF-REPORT.{GPT_DEPLOYMENT}.csv\", index=True)\n",
    "final_df_irae_eval.to_csv(f\"{path_eval}PATIENT-EVAL.FULL.DETAILED-REPORT.{GPT_DEPLOYMENT}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation [large]: filtered irAE large labels + best threshold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_llm_large_best_threshold = convert_specific_large(df_llm_full_best_threshold, dict_map_full2large, filter_list_irae_full, filter_list_irae_large)\n",
    "\n",
    "df_y_gold = df_gold_large[filter_list_irae_large].to_numpy().astype(int)\n",
    "df_y_llm = df_llm_large_best_threshold[filter_list_irae_large].to_numpy().astype(int)    \n",
    "\n",
    "final_clf_report = classification_report(df_y_gold, df_y_llm, target_names = filter_list_irae_large, zero_division=0, output_dict=True)\n",
    "final_clf_report = pd.DataFrame(final_clf_report).transpose()\n",
    "display(final_clf_report)\n",
    "\n",
    "final_df_irae_eval = IRAEUtils.irae_eval(df_y_gold, df_y_llm, filter_list_irae_large)\n",
    "display(final_df_irae_eval)\n",
    "\n",
    "final_clf_report.to_csv(f\"{path_eval}PATIENT-EVAL-LARGE.CLF-REPORT.{GPT_DEPLOYMENT}.csv\", index=True)\n",
    "final_df_irae_eval.to_csv(f\"{path_eval}PATIENT-EVAL-LARGE.DETAILED-REPORT.{GPT_DEPLOYMENT}.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
