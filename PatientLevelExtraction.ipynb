{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from openai import AzureOpenAI                      \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys, time, random, math, os, json, spacy\n",
    "import IRAEUtils\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, make_scorer, roc_auc_score, roc_curve\n",
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config paramaters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPT_DEPLOYMENT = \"OAI05-GPT35Turbo16-0613_061823\"\n",
    "#GPT_DEPLOYMENT = \"V04-GPT4Turbo-2024-04-09\"\n",
    "GPT_DEPLOYMENT = \"V05-GPT4o\"\n",
    "\n",
    "#IRAE_LABEL_COUNT = 0\n",
    "IRAE_LABEL_COUNT = 1\n",
    "\n",
    "IRAE_LABEL_COUNT_MAX = 11\n",
    "\n",
    "BATCH = \"TEST\"\n",
    "#BATCH = \"04\"\n",
    "\n",
    "project_path = os.getenv(\"VU_PROJ_PATH\") + \"immunotoxicity/\"\n",
    "notes_path = project_path + \"out/llm/notes/\" \n",
    "#notes_path = project_path + \"out/llm/notes/notestest/\" \n",
    "\n",
    "irae_anno_map_path = f\"{project_path}out/llm/notes/batches/frozen/Map.person-level.Batch.{BATCH}.irAE.csv\"\n",
    "irae_synset_map_path = project_path + \"out/llm/notes/batches/frozen/Map.irae.synset.FINAL.csv\"\n",
    "\n",
    "#path_eval = f\"{project_path}out/llm/eval-patient-level/test/{GPT_DEPLOYMENT}/\"\n",
    "path_eval = f\"{project_path}out/llm/eval-patient-level/{GPT_DEPLOYMENT}/\"\n",
    "\n",
    "llm = AzureOpenAI(\n",
    "  api_key = os.getenv(\"AZURE_OPENAI05_API_KEY\"),  \n",
    "  api_version =  \"2024-07-01-preview\",  \n",
    "  azure_endpoint = os.getenv(\"AZURE_OPENAI05_ENDPOINT\")\n",
    ")\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "print(irae_anno_map_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Header: 'GRID', 'PID', 'AgeFirstICI', 'RaceEth', 'Gender', 'ICIType', 'CancerType', 'FolderName', [irAEs]\n",
    "df_irae_data = pd.read_csv(irae_anno_map_path)\n",
    "#display(df_irae_data)\n",
    "\n",
    "# Load irAE list\n",
    "list_irae_label = df_irae_data.columns.tolist()\n",
    "list_irae_label = list_irae_label[8:] # exclude non-irAE data\n",
    "\n",
    "# data frame with manual/gold irAE labels\n",
    "df_manual_GRID_IRAE = df_irae_data[['GRID'] + list_irae_label]\n",
    "#display(df_manual_GRID_IRAE)\n",
    "\n",
    "# ICI list\n",
    "list_ici_label = \"atezolizumab (tecentriq, atezo), avelumab (bavencio), durvalumab (imfinzi), ipilimumab (yervoy), nivolumab (opdivo, nivo), pembrolizumab (keytruda, pembro)\"\n",
    "\n",
    "# Load a dictionary mapping irAEs to its variations\n",
    "map_irae_label_expand = IRAEUtils.read2cols_2dict_all(irae_synset_map_path, 0, 1, \"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def folder_size() :\n",
    "    folder_size = 0\n",
    "    for folder_name in df_irae_data['FolderName'].tolist() :\n",
    "        folder_size += len(IRAEUtils.file_names(notes_path + \"/\" + folder_name))\n",
    "    return folder_size\n",
    "\n",
    "print(folder_size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IRAE prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt instructions for each irAE type. The output is requested in JSON format\n",
    "# Relaace get_irae_list_json\n",
    "def json_format_irae_list(list_irae_label):\n",
    "    s = '{'\n",
    "    for irae_label in list_irae_label :\n",
    "        s += f'''\\n\"{irae_label}\": Output 'Yes' if the patient has experienced {map_irae_label_expand[irae_label]} because of exposure to one or more immune checkpoint inhibitors. Otherwise, output 'No'.,'''\n",
    "    s += '}'\n",
    "\n",
    "    return s\n",
    "\n",
    "#print(get_irae_list_json(irae_label_list))\n",
    "\n",
    "# Zero-shot prompt template \n",
    "def irae_prompt(note_text, list_ici_label, list_irae_label):\n",
    "    irae_json_format = json_format_irae_list(list_irae_label)\n",
    "    messages = []\n",
    "    messages.append({\"role\": \"system\", \"content\": f\"\"\"You are a clinical expert in identifying immune-related adverse events (irAEs) caused by immune checkpoint inhibitors (ICIs).                                           \n",
    "                     You will receive as input a patient note corresponding to a patient who was treated or is currently treated with one or multiple immune checkpoint inhibitors (ICIs) from the following ICI list: {list_ici_label}. \n",
    "                     Your task is to determine if the patient note describes any of the immune-related adverse events (irAEs) experienced by the patient and caused by immune checkpoint inhibitors.\n",
    "                     Output your response in a JSON format using the following structure: \n",
    "                     {irae_json_format}\"\"\"})\n",
    "    messages.append({\"role\": \"user\", \"content\": f\"\"\"Does the following patient note describe immune-related adverse \n",
    "                     events experienced by the patient? \n",
    "                     Patient note: {note_text}\"\"\"})                     \n",
    "    return messages\n",
    "\n",
    "print(irae_prompt(\" .. note .. \", list_ici_label, list_irae_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM output processisng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in:  [note-level] dictionary of yes/no reponses by LLM for each irAE label\n",
    "# out: [note-level] dictionary of 1/0 reponses by LLM for each irAE label\n",
    "#\n",
    "# note: yes/no conversion to 1/0 imvolves NLP processing\n",
    "def convert_llmresponse_yn_to_01(json_dict_irae_llmreponse_yn, list_irae_label, nlp) :\n",
    "    dict_llmresponse01 = dict()\n",
    "\n",
    "    if len(list_irae_label) != len(json_dict_irae_llmreponse_yn):\n",
    "        #print(f\"FALSE MSG len{}\")\n",
    "        return False, dict_llmresponse01\n",
    "\n",
    "    for irae_label in list_irae_label :\n",
    "        if irae_label not in json_dict_irae_llmreponse_yn :            \n",
    "            return False, dict_llmresponse01\n",
    "\n",
    "        llmresponse01 = IRAEUtils.convert_llm_response_to_01(json_dict_irae_llmreponse_yn[irae_label], nlp)\n",
    "        #print(f\"{irae_label} : {json_dict_irae_reponse[irae_label]} : {response_01_elem}\")        \n",
    "        dict_llmresponse01[irae_label] = llmresponse01\n",
    "        \n",
    "        if llmresponse01 == -1 :\n",
    "            return False, dict_llmresponse01\n",
    "    \n",
    "    return True, dict_llmresponse01\n",
    "\n",
    "\n",
    "# in:  [patient-level] list of LLM responses in yes/no JSON format corresponding to all notes of a given patient \n",
    "# out: [patient-level] data frame with 1/0 responses. Each row corresponds to a note. Each colum corresponds to an irAE label.\n",
    "def convert_list_llmresponse_yn_to_01(list_note_fnames, list_note_llmresponse_yn, list_irae_label, nlp) :\n",
    "    patient_multirow_df_llmresponse_01 = pd.DataFrame(columns=list_irae_label)\n",
    "    list_note_fnames_filter = [] \n",
    "\n",
    "    for note_fname, llmresponse_yn  in zip(list_note_fnames, list_note_llmresponse_yn) :\n",
    "        #print(f\"LL response: {llmresponse_yn}\")\n",
    "        json_llm_response_yn = llmresponse_yn.removeprefix(\"```json\").removesuffix(\"```\").strip()\n",
    "        #print(f\"LLM response: {json_llm_response_yn}\")\n",
    "        if IRAEUtils.is_json(json_llm_response_yn) :\n",
    "            json_dict_irae_reponse = json.loads(json_llm_response_yn)\n",
    "            #print(json_dict_irae_reponse)\n",
    "            flag, dict_llm_response01 = convert_llmresponse_yn_to_01(json_dict_irae_reponse, list_irae_label, nlp)\n",
    "            if flag == True :\n",
    "                list_note_fnames_filter.append(note_fname)\n",
    "                #print(llm_response_01_dict)\n",
    "                note_df_llmresponse01 = pd.DataFrame([dict_llm_response01])\n",
    "                #display(note_df_llmresponse01)                \n",
    "                patient_multirow_df_llmresponse_01 = pd.concat([patient_multirow_df_llmresponse_01, note_df_llmresponse01], ignore_index=True)\n",
    "                #display(patient_df_llmresponse_01)\n",
    "            #else :\n",
    "            #    print(f\"FALSE RETURN:{json_dict_irae_reponse}\")\n",
    "    \n",
    "    patient_df_notefname = pd.DataFrame(list_note_fnames_filter, columns=['FileName'])\n",
    "    return patient_df_notefname, patient_multirow_df_llmresponse_01\n",
    "\n",
    "# in: multi-row df of binary llmreposes for all patient notes\n",
    "# out: one-row df with collapsed llm responses\n",
    "#\n",
    "# Note: The assignment of an irAE label at patient-level was determined if the irAE label was prediced for at least one note\n",
    "def collapse_llresponses01_notes_to_patient(patient_multirow_df_llmresponse_01) :\n",
    "    dict_collapsed_llmrespose01 = dict()\n",
    "    for colname in patient_multirow_df_llmresponse_01.columns:\n",
    "        column_sum = patient_multirow_df_llmresponse_01[colname].sum()\n",
    "        binary_value = 1 if column_sum > IRAE_LABEL_COUNT else 0\n",
    "        dict_collapsed_llmrespose01[colname] = binary_value\n",
    "    \n",
    "    patient_onerow_df_llmresponse_01 = pd.DataFrame([dict_collapsed_llmrespose01])\n",
    "    return patient_onerow_df_llmresponse_01\n",
    "\n",
    "# generalization of collapse_llresponses01_notes_to_patient for multiple threshold values\n",
    "def collapse_llresponses01_notes_to_patient_multi_th(patient_multirow_df_llmresponse_01) :\n",
    "    # init the list of dictionaries\n",
    "    list_dict_collapsed_llmrespose01_multi_th = []    \n",
    "    for _ in range(IRAE_LABEL_COUNT_MAX):        \n",
    "        list_dict_collapsed_llmrespose01_multi_th.append({})\n",
    "    \n",
    "    # update the list of dictionaries\n",
    "    for colname in patient_multirow_df_llmresponse_01.columns:\n",
    "        column_sum = patient_multirow_df_llmresponse_01[colname].sum()\n",
    "        for threshold in range(IRAE_LABEL_COUNT_MAX):\n",
    "            list_dict_collapsed_llmrespose01_multi_th[threshold][colname] = 1 if column_sum > threshold else 0\n",
    "\n",
    "    # init the list of 1-row data frames\n",
    "    list_patient_onerow_df_llmresponse01_multi_th = []\n",
    "    for threshold in range(IRAE_LABEL_COUNT_MAX):\n",
    "        list_patient_onerow_df_llmresponse01_multi_th.append(pd.DataFrame([list_dict_collapsed_llmrespose01_multi_th[threshold]]))\n",
    "\n",
    "    return list_patient_onerow_df_llmresponse01_multi_th"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run LLMs - multiple thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exception_list = []\n",
    "list_patient_id = []\n",
    "patient_counter = 0\n",
    "# patient_df_llmresponse_01 > list_patient_df_llmresponse01_multi_th\n",
    "list_patient_df_llmresponse01_multi_th = [] # init w/ empty dfs\n",
    "for threshold in range(IRAE_LABEL_COUNT_MAX):\n",
    "    list_patient_df_llmresponse01_multi_th.append(pd.DataFrame(columns=list_irae_label))\n",
    "\n",
    "for patient_id, patient_folder in zip (df_irae_data['GRID'], df_irae_data['FolderName']) :\n",
    "    list_patient_id.append(patient_id)\n",
    "    patient_counter += 1\n",
    "    print(f\"{patient_counter} - {patient_id} - {patient_folder}\")\n",
    "    list_patient_note_llmresponse_yn = []\n",
    "    list_patient_note_fname = IRAEUtils.file_names(notes_path + patient_folder)\n",
    "    for patient_note_fname in  list_patient_note_fname:\n",
    "        print('.', end='', flush=True)\n",
    "        #print(\"Patient note:\" + patient_note_fname)\n",
    "        note = IRAEUtils.read(notes_path + patient_folder + \"/\" + patient_note_fname)\n",
    "        #print(irae_prompt(note, list_ici_label, list_irae_label))\n",
    "\n",
    "        try:\n",
    "            llm_response = llm.chat.completions.create(model = GPT_DEPLOYMENT,\n",
    "                                                       temperature=0.0, max_tokens=1000, n = 1,\n",
    "                                                       frequency_penalty=0, presence_penalty=0, seed = 13,     \n",
    "                                                       #top_p=1, ## reco: alter this param or temp but not both https://platform.openai.com/docs/api-reference/chat/create            \n",
    "                                                       messages = irae_prompt(note, list_ici_label, list_irae_label))\n",
    "            #print(llm_response.choices[0].message.content.strip())\n",
    "            list_patient_note_llmresponse_yn.append(llm_response.choices[0].message.content.strip())\n",
    "        except Exception as e:\n",
    "            message = str(e).strip().replace('\\n', ' ')\n",
    "            print(\"LLMException: \"+message)\n",
    "            list_patient_note_llmresponse_yn.append(\"LLMException: \"+message)\n",
    "            exception_list.append(f\"LLMException: [{patient_note_fname}] {message}\")\n",
    "\n",
    "    # write NoteFName - llm-responses in yes/no JSON format\n",
    "    df_notes_fname_llm_yn = pd.DataFrame({'FileName' : list_patient_note_fname, 'LLM_yn' : list_patient_note_llmresponse_yn})\n",
    "    df_notes_fname_llm_yn.to_csv(f\"{path_eval}OUT.{patient_counter}.{patient_id}.note-level-llm-yn.B.{BATCH}.csv\", index=False)\n",
    "\n",
    "    # convert the llm yes/no llm responses into binary responses        \n",
    "    patient_df_filter_notefname, patient_df_filter_llmresponse_01 = convert_list_llmresponse_yn_to_01(list_patient_note_fname, list_patient_note_llmresponse_yn, list_irae_label, nlp)\n",
    "    df_notes_fname_llm_01 = pd.concat([patient_df_filter_notefname, patient_df_filter_llmresponse_01], axis=1)    \n",
    "    df_notes_fname_llm_01.to_csv(f\"{path_eval}OUT.{patient_counter}.{patient_id}.note-level-llm-01.B.{BATCH}.csv\", index=False)\n",
    "\n",
    "    ##------\n",
    "    ##\n",
    "    ##\n",
    "    # collapse the binary llm responses from note- to patient-level\n",
    "    #\n",
    "    # patient_onerow_df_llmresponse_01 >> list_patient_onerow_df_llmresponse01_multi_th\n",
    "    # patient_df_llmresponse_01 >> list_patient_df_llmresponse01_multi_th\n",
    "    #\n",
    "    list_patient_onerow_df_llmresponse01_multi_th = collapse_llresponses01_notes_to_patient_multi_th(patient_df_filter_llmresponse_01)\n",
    "\n",
    "    for threshold in range(IRAE_LABEL_COUNT_MAX):\n",
    "        list_patient_onerow_df_llmresponse01_multi_th[threshold].to_csv(f\"{path_eval}OUT.{patient_counter}.{patient_id}.patient-level-llm-01.TH.{threshold}.B.{BATCH}.csv\", index=False)\n",
    "\n",
    "    # append patient llm response\n",
    "    for threshold in range(IRAE_LABEL_COUNT_MAX):\n",
    "        list_patient_df_llmresponse01_multi_th[threshold] = pd.concat([list_patient_df_llmresponse01_multi_th[threshold], list_patient_onerow_df_llmresponse01_multi_th[threshold]], ignore_index=True)\n",
    "    \n",
    "    # write the current llresponses\n",
    "    current_df_patient_id = pd.DataFrame(list_patient_id, columns=['GRID'])\n",
    "    \n",
    "    list_current_patient_df_llmresponse01_multi_th = []\n",
    "    for threshold in range(IRAE_LABEL_COUNT_MAX):\n",
    "        list_current_patient_df_llmresponse01_multi_th.append(pd.concat([current_df_patient_id, list_patient_df_llmresponse01_multi_th[threshold]], axis=1))\n",
    "    \n",
    "    for threshold in range(IRAE_LABEL_COUNT_MAX):\n",
    "        list_current_patient_df_llmresponse01_multi_th[threshold].to_csv(f\"{path_eval}IRAE-LABELS.CRT.{patient_counter}.LLM.patient-level.TH.{threshold}.B.{BATCH}.{GPT_DEPLOYMENT}.csv\", index=False)\n",
    "\n",
    "    # write the current gold/manual responses\n",
    "    current_df_manual_GRID_IRAE = pd.merge(current_df_patient_id, df_manual_GRID_IRAE, on='GRID', how='inner')\n",
    "    current_df_manual_GRID_IRAE.to_csv(f\"{path_eval}IRAE-LABELS.CRT.{patient_counter}.GOLD.patient-level.B.{BATCH}.{GPT_DEPLOYMENT}.csv\", index=False)\n",
    "\n",
    "    for threshold in range(IRAE_LABEL_COUNT_MAX):\n",
    "        if (current_df_manual_GRID_IRAE['GRID'] != list_current_patient_df_llmresponse01_multi_th[threshold]['GRID']).any() :\n",
    "            raise Exception(f\"GRID columns are not equal patient counter({patient_counter}) threshold({threshold})\")\n",
    "\n",
    "\n",
    "    # current eval\n",
    "    current_df_y_gold = current_df_manual_GRID_IRAE[list_irae_label].to_numpy().astype(int)\n",
    "\n",
    "    for threshold in range(IRAE_LABEL_COUNT_MAX):\n",
    "        current_df_y_llm = list_current_patient_df_llmresponse01_multi_th[threshold][list_irae_label].to_numpy().astype(int)\n",
    "\n",
    "        # classif report\n",
    "        current_clf_report = classification_report(current_df_y_gold, current_df_y_llm, target_names=list_irae_label, zero_division=0, output_dict=True)\n",
    "        current_df_report = pd.DataFrame(current_clf_report).transpose()\n",
    "        current_df_report.to_csv(f\"{path_eval}EVAL.CRT.{patient_counter}.CLF-REPORT.TH.{threshold}.B.{BATCH}.{GPT_DEPLOYMENT}.csv\", index=True)\n",
    "\n",
    "        # write detailed eval\n",
    "        current_df_irae_eval = IRAEUtils.irae_eval(current_df_y_gold, current_df_y_llm, list_irae_label)\n",
    "        current_df_irae_eval.to_csv(f\"{path_eval}EVAL.CRT.{patient_counter}.DETAILED-REPORT.TH.{threshold}.B.{BATCH}.{GPT_DEPLOYMENT}.csv\", index=False)\n",
    "\n",
    "# write final llm resposens01 at patient level\n",
    "df_grid = df_irae_data[['GRID']]\n",
    "\n",
    "list_df_llm_data_multi_th = []\n",
    "for threshold in range(IRAE_LABEL_COUNT_MAX):\n",
    "    list_df_llm_data_multi_th.append(pd.concat([df_grid, list_patient_df_llmresponse01_multi_th[threshold]], axis=1))\n",
    "\n",
    "for threshold in range(IRAE_LABEL_COUNT_MAX):\n",
    "    list_df_llm_data_multi_th[threshold].to_csv(f\"{path_eval}IRAE-LABELS.FINAL.Y_LLM.TH.{threshold}.B.{BATCH}.patient-level.{GPT_DEPLOYMENT}.csv\", index=False)\n",
    "\n",
    "\n",
    "# write final gold IRAE labels at patient level\n",
    "df_manual_GRID_IRAE.to_csv(f\"{path_eval}IRAE-LABELS.FINAL.Y_GOLD.B.{BATCH}.patient-level.{GPT_DEPLOYMENT}.csv\", index=False)\n",
    "\n",
    "# final classif report and detailed eval\n",
    "df_y_gold = df_irae_data[list_irae_label].to_numpy().astype(int)\n",
    "\n",
    "for threshold in range(IRAE_LABEL_COUNT_MAX):\n",
    "    #df_y_llm = df_llm_data[list_irae_label].to_numpy().astype(int)\n",
    "    df_y_llm = list_df_llm_data_multi_th[threshold][list_irae_label].to_numpy().astype(int)\n",
    "\n",
    "    final_clf_report = classification_report(df_y_gold, df_y_llm, target_names=list_irae_label, zero_division=0, output_dict=True)\n",
    "    final_df_report = pd.DataFrame(final_clf_report).transpose()\n",
    "    final_df_report.to_csv(f\"{path_eval}_FINAL-EVAL.CLF-REPORT.TH.{threshold}.B.{BATCH}.{GPT_DEPLOYMENT}.csv\", index=True)\n",
    "\n",
    "    final_df_irae_eval = IRAEUtils.irae_eval(df_y_gold, df_y_llm, list_irae_label)\n",
    "    final_df_irae_eval.to_csv(f\"{path_eval}_FINAL-EVAL.CRT.DETAILED-REPORT.TH.{threshold}.B.{BATCH}.{GPT_DEPLOYMENT}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run LLMs - single threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exception_list = []\n",
    "list_patient_id = []\n",
    "patient_counter = 0\n",
    "patient_df_llmresponse_01 = pd.DataFrame(columns=list_irae_label)\n",
    "for patient_id, patient_folder in zip (df_irae_data['GRID'], df_irae_data['FolderName']) :\n",
    "    list_patient_id.append(patient_id)\n",
    "    patient_counter += 1\n",
    "    print(f\"{patient_counter} - {patient_id} - {patient_folder}\")\n",
    "    list_patient_note_llmresponse_yn = []\n",
    "    list_patient_note_fname = IRAEUtils.file_names(notes_path + patient_folder)\n",
    "    for patient_note_fname in  list_patient_note_fname:\n",
    "        print('.', end='', flush=True)\n",
    "        #print(\"Patient note:\" + patient_note_fname)\n",
    "        note = IRAEUtils.read(notes_path + patient_folder + \"/\" + patient_note_fname)\n",
    "        #print(irae_prompt(note, list_ici_label, list_irae_label))\n",
    "\n",
    "        try:\n",
    "            llm_response = llm.chat.completions.create(model = GPT_DEPLOYMENT,\n",
    "                                                       temperature=0.0, max_tokens=1000, n = 1,\n",
    "                                                       frequency_penalty=0, presence_penalty=0, seed = 13,     \n",
    "                                                       #top_p=1, ## reco: alter this param or temp but not both https://platform.openai.com/docs/api-reference/chat/create            \n",
    "                                                       messages = irae_prompt(note, list_ici_label, list_irae_label))\n",
    "            #print(llm_response.choices[0].message.content.strip())\n",
    "            list_patient_note_llmresponse_yn.append(llm_response.choices[0].message.content.strip())\n",
    "        except Exception as e:\n",
    "            message = str(e).strip().replace('\\n', ' ')\n",
    "            print(\"LLMException: \"+message)\n",
    "            list_patient_note_llmresponse_yn.append(\"LLMException: \"+message)\n",
    "            exception_list.append(f\"LLMException: [{patient_note_fname}] {message}\")\n",
    "\n",
    "    # write NoteFName - llm-responses in yes/no JSON format\n",
    "    df_notes_fname_llm_yn = pd.DataFrame({'FileName' : list_patient_note_fname, 'LLM_yn' : list_patient_note_llmresponse_yn})\n",
    "    df_notes_fname_llm_yn.to_csv(f\"{path_eval}OUT.{patient_counter}.{patient_id}.note-level-llm-yn.TH.{IRAE_LABEL_COUNT}.B.{BATCH}.csv\", index=False)\n",
    "\n",
    "    # convert the llm yes/no llm responses into binary responses        \n",
    "    patient_df_filter_notefname, patient_df_filter_llmresponse_01 = convert_list_llmresponse_yn_to_01(list_patient_note_fname, list_patient_note_llmresponse_yn, list_irae_label, nlp)\n",
    "    df_notes_fname_llm_01 = pd.concat([patient_df_filter_notefname, patient_df_filter_llmresponse_01], axis=1)    \n",
    "    df_notes_fname_llm_01.to_csv(f\"{path_eval}OUT.{patient_counter}.{patient_id}.note-level-llm-01.TH.{IRAE_LABEL_COUNT}.B.{BATCH}.csv\", index=False)\n",
    "\n",
    "    # collapse the binary llm responses from note- to patient-level\n",
    "    patient_onerow_df_llmresponse_01 = collapse_llresponses01_notes_to_patient(patient_df_filter_llmresponse_01)\n",
    "    patient_onerow_df_llmresponse_01.to_csv(f\"{path_eval}OUT.{patient_counter}.{patient_id}.patient-level-llm-01.TH.{IRAE_LABEL_COUNT}.B.{BATCH}.csv\", index=False)\n",
    "\n",
    "    # append patient llm response\n",
    "    patient_df_llmresponse_01 = pd.concat([patient_df_llmresponse_01, patient_onerow_df_llmresponse_01], ignore_index=True)\n",
    "\n",
    "    # write the current llresponses\n",
    "    current_df_patient_id = pd.DataFrame(list_patient_id, columns=['GRID'])\n",
    "    current_patient_df_llmresponse_01 = pd.concat([current_df_patient_id, patient_df_llmresponse_01], axis=1)\n",
    "    current_patient_df_llmresponse_01.to_csv(f\"{path_eval}IRAE-LABELS.CRT.{patient_counter}.LLM.patient-level.TH.{IRAE_LABEL_COUNT}.B.{BATCH}.{GPT_DEPLOYMENT}.csv\", index=False)\n",
    "\n",
    "    # write the current gold/manual responses\n",
    "    current_df_manual_GRID_IRAE = pd.merge(current_df_patient_id, df_manual_GRID_IRAE, on='GRID', how='inner')\n",
    "    current_df_manual_GRID_IRAE.to_csv(f\"{path_eval}IRAE-LABELS.CRT.{patient_counter}.GOLD.patient-level.TH.{IRAE_LABEL_COUNT}.B.{BATCH}.{GPT_DEPLOYMENT}.csv\", index=False)\n",
    "\n",
    "    if (current_df_manual_GRID_IRAE['GRID'] != current_patient_df_llmresponse_01['GRID']).any() :\n",
    "        raise Exception(f\"GRID columns are not equal ({patient_counter})\")\n",
    "\n",
    "    # current data for eval\n",
    "    current_df_y_gold = current_df_manual_GRID_IRAE[list_irae_label].to_numpy().astype(int)\n",
    "    current_df_y_llm = current_patient_df_llmresponse_01[list_irae_label].to_numpy().astype(int)\n",
    "\n",
    "    # write classif report\n",
    "    current_clf_report = classification_report(current_df_y_gold, current_df_y_llm, target_names=list_irae_label, zero_division=0, output_dict=True)\n",
    "    current_df_report = pd.DataFrame(current_clf_report).transpose()\n",
    "    current_df_report.to_csv(f\"{path_eval}EVAL.CRT.{patient_counter}.CLF-REPORT.TH.{IRAE_LABEL_COUNT}.B.{BATCH}.{GPT_DEPLOYMENT}.csv\", index=True)\n",
    "\n",
    "    # write detailed eval\n",
    "    current_df_irae_eval = IRAEUtils.irae_eval(current_df_y_gold, current_df_y_llm, list_irae_label)\n",
    "    current_df_irae_eval.to_csv(f\"{path_eval}EVAL.CRT.{patient_counter}.DETAILED-REPORT.TH.{IRAE_LABEL_COUNT}.B.{BATCH}.{GPT_DEPLOYMENT}.csv\", index=False)\n",
    "\n",
    "# write final llm resposens01 at patient level\n",
    "df_grid = df_irae_data[['GRID']]\n",
    "df_llm_data = pd.concat([df_grid, patient_df_llmresponse_01], axis=1)\n",
    "df_llm_data.to_csv(f\"{path_eval}IRAE-LABELS.FINAL.Y_LLM.TH.{IRAE_LABEL_COUNT}.B.{BATCH}.patient-level.{GPT_DEPLOYMENT}.csv\", index=False)\n",
    "\n",
    "# write final gold IRAE labels at patient level\n",
    "df_manual_GRID_IRAE.to_csv(f\"{path_eval}IRAE-LABELS.FINAL.Y_GOLD.TH.{IRAE_LABEL_COUNT}.B.{BATCH}.patient-level.{GPT_DEPLOYMENT}.csv\", index=False)\n",
    "\n",
    "# write final classif report and detailed eval\n",
    "df_y_gold = df_irae_data[list_irae_label].to_numpy().astype(int)\n",
    "df_y_llm = df_llm_data[list_irae_label].to_numpy().astype(int)\n",
    "\n",
    "final_clf_report = classification_report(df_y_gold, df_y_llm, target_names=list_irae_label, zero_division=0, output_dict=True)\n",
    "final_df_report = pd.DataFrame(final_clf_report).transpose()\n",
    "final_df_report.to_csv(f\"{path_eval}_FINAL-EVAL.CLF-REPORT.TH.{IRAE_LABEL_COUNT}.B.{BATCH}.{GPT_DEPLOYMENT}.csv\", index=True)\n",
    "\n",
    "final_df_irae_eval = IRAEUtils.irae_eval(df_y_gold, df_y_llm, list_irae_label)\n",
    "final_df_irae_eval.to_csv(f\"{path_eval}_FINAL-EVAL.CRT.DETAILED-REPORT.TH.{IRAE_LABEL_COUNT}.B.{BATCH}.{GPT_DEPLOYMENT}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_df_irae_eval = IRAEUtils.irae_eval(current_df_y_gold, current_df_y_llm, list_irae_label)\n",
    "print(current_df_irae_eval)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
